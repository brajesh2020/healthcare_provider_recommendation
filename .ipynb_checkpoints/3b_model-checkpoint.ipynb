{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9d099a3-de96-4355-97bc-b1c85ce53101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas lib as pd\n",
    "import pandas as pd\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# read by default 1st sheet of an excel file\n",
    "# dataframe1 = pd.read_excel('E://REVA//Healthcare Providers Data For Anomaly Detection//Medicare1//2021_CA_Data.xlsx',sheet_name=\"Sheet1\",header=0)\n",
    "dataframe1 = pd.read_excel('E://GitHub//healthcare_provider_recommendation//2021_CA_Data.xlsx',sheet_name=\"data1\",header=0)\n",
    "\n",
    "# df1 = dataframe1[dataframe1['Rndrng_Prvdr_Zip5'] == 90095] #user input\n",
    "# df2=df1[df1[\"HCPCS_Desc\"].str.contains(\"patient\")]\n",
    "# df2 = df2[df2['HCPCS_Cd'] == 99205] #user input\n",
    "# df2 = df2[df2['Rndrng_Prvdr_Type'] == 'Neurology'] #user input\n",
    "df2['Rndrng_Prvdr_Zip5_str'] = df2['Rndrng_Prvdr_Zip5'].astype(str)\n",
    "df2['HCPCS_Cd_str'] = df2['HCPCS_Cd'].astype(str)\n",
    "df2['Rndrng_Prvdr_Type_str'] = df2['Rndrng_Prvdr_Type'].astype(str)\n",
    "\n",
    "# Concatenate the values of 'Column1' and 'Column2' and store the result in a new column 'Concatenated_Column'\n",
    "df2['combined_cat_column'] = df2['Rndrng_Prvdr_Zip5_str'] + df2['HCPCS_Cd_str'] + df2['Rndrng_Prvdr_Type_str']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44d88120-a3cc-4016-97e9-c519d33dfd06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rndrng_NPI</th>\n",
       "      <th>Rndrng_Prvdr_Last_Org_Name</th>\n",
       "      <th>Rndrng_Prvdr_First_Name</th>\n",
       "      <th>Rndrng_Prvdr_MI</th>\n",
       "      <th>Rndrng_Prvdr_Crdntls</th>\n",
       "      <th>Rndrng_Prvdr_Gndr</th>\n",
       "      <th>Rndrng_Prvdr_Ent_Cd</th>\n",
       "      <th>Rndrng_Prvdr_St1</th>\n",
       "      <th>Rndrng_Prvdr_St2</th>\n",
       "      <th>Rndrng_Prvdr_City</th>\n",
       "      <th>...</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LNG</th>\n",
       "      <th>HCPCS_category</th>\n",
       "      <th>HCPCS_sub category</th>\n",
       "      <th>Rndrng_Prvdr_Zip5_str</th>\n",
       "      <th>HCPCS_Cd_str</th>\n",
       "      <th>Rndrng_Prvdr_Type_str</th>\n",
       "      <th>combined_cat_column</th>\n",
       "      <th>Reverse_Avg_Sbmtd_Chrg</th>\n",
       "      <th>sum_beneficiary_days_and_rev_avg_charge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20647</th>\n",
       "      <td>1457557910</td>\n",
       "      <td>Frechette</td>\n",
       "      <td>Eric</td>\n",
       "      <td>S</td>\n",
       "      <td>M.D.</td>\n",
       "      <td>M</td>\n",
       "      <td>I</td>\n",
       "      <td>300 Medical Plaza # B200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>...</td>\n",
       "      <td>34.0712</td>\n",
       "      <td>-118.443523</td>\n",
       "      <td>New patient outpatient visit</td>\n",
       "      <td>, total time 60-74 minutes</td>\n",
       "      <td>90095</td>\n",
       "      <td>99205</td>\n",
       "      <td>Neurology</td>\n",
       "      <td>9009599205Neurology</td>\n",
       "      <td>7651.00</td>\n",
       "      <td>7866.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20578</th>\n",
       "      <td>1104960517</td>\n",
       "      <td>Dorsch</td>\n",
       "      <td>Andrew</td>\n",
       "      <td>K</td>\n",
       "      <td>MD</td>\n",
       "      <td>M</td>\n",
       "      <td>I</td>\n",
       "      <td>710 Westwood Plz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>...</td>\n",
       "      <td>34.0712</td>\n",
       "      <td>-118.443523</td>\n",
       "      <td>New patient outpatient visit</td>\n",
       "      <td>, total time 60-74 minutes</td>\n",
       "      <td>90095</td>\n",
       "      <td>99205</td>\n",
       "      <td>Neurology</td>\n",
       "      <td>9009599205Neurology</td>\n",
       "      <td>7536.00</td>\n",
       "      <td>7579.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20649</th>\n",
       "      <td>1457731986</td>\n",
       "      <td>Podraza</td>\n",
       "      <td>Katherine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>I</td>\n",
       "      <td>300 Ucla Medical Plz # B200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>...</td>\n",
       "      <td>34.0712</td>\n",
       "      <td>-118.443523</td>\n",
       "      <td>New patient outpatient visit</td>\n",
       "      <td>, total time 60-74 minutes</td>\n",
       "      <td>90095</td>\n",
       "      <td>99205</td>\n",
       "      <td>Neurology</td>\n",
       "      <td>9009599205Neurology</td>\n",
       "      <td>7289.48</td>\n",
       "      <td>7314.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20629</th>\n",
       "      <td>1376842898</td>\n",
       "      <td>Darby</td>\n",
       "      <td>Adam</td>\n",
       "      <td>J</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>I</td>\n",
       "      <td>300 Ucla Medical Plz</td>\n",
       "      <td>Suite B200</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>...</td>\n",
       "      <td>34.0712</td>\n",
       "      <td>-118.443523</td>\n",
       "      <td>New patient outpatient visit</td>\n",
       "      <td>, total time 60-74 minutes</td>\n",
       "      <td>90095</td>\n",
       "      <td>99205</td>\n",
       "      <td>Neurology</td>\n",
       "      <td>9009599205Neurology</td>\n",
       "      <td>6641.00</td>\n",
       "      <td>6801.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20685</th>\n",
       "      <td>1639194038</td>\n",
       "      <td>Ishiyama</td>\n",
       "      <td>Gail</td>\n",
       "      <td>P</td>\n",
       "      <td>MD</td>\n",
       "      <td>F</td>\n",
       "      <td>I</td>\n",
       "      <td>300 Medical Plaza</td>\n",
       "      <td>#b200</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>...</td>\n",
       "      <td>34.0712</td>\n",
       "      <td>-118.443523</td>\n",
       "      <td>New patient outpatient visit</td>\n",
       "      <td>, total time 60-74 minutes</td>\n",
       "      <td>90095</td>\n",
       "      <td>99205</td>\n",
       "      <td>Neurology</td>\n",
       "      <td>9009599205Neurology</td>\n",
       "      <td>6641.00</td>\n",
       "      <td>6783.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Rndrng_NPI Rndrng_Prvdr_Last_Org_Name Rndrng_Prvdr_First_Name  \\\n",
       "20647  1457557910                  Frechette                    Eric   \n",
       "20578  1104960517                     Dorsch                  Andrew   \n",
       "20649  1457731986                    Podraza               Katherine   \n",
       "20629  1376842898                      Darby                    Adam   \n",
       "20685  1639194038                   Ishiyama                    Gail   \n",
       "\n",
       "      Rndrng_Prvdr_MI Rndrng_Prvdr_Crdntls Rndrng_Prvdr_Gndr  \\\n",
       "20647               S                 M.D.                 M   \n",
       "20578               K                   MD                 M   \n",
       "20649             NaN                  NaN                 F   \n",
       "20629               J                  NaN                 M   \n",
       "20685               P                   MD                 F   \n",
       "\n",
       "      Rndrng_Prvdr_Ent_Cd             Rndrng_Prvdr_St1 Rndrng_Prvdr_St2  \\\n",
       "20647                   I     300 Medical Plaza # B200              NaN   \n",
       "20578                   I             710 Westwood Plz              NaN   \n",
       "20649                   I  300 Ucla Medical Plz # B200              NaN   \n",
       "20629                   I         300 Ucla Medical Plz       Suite B200   \n",
       "20685                   I            300 Medical Plaza            #b200   \n",
       "\n",
       "      Rndrng_Prvdr_City  ...      LAT         LNG  \\\n",
       "20647       Los Angeles  ...  34.0712 -118.443523   \n",
       "20578       Los Angeles  ...  34.0712 -118.443523   \n",
       "20649       Los Angeles  ...  34.0712 -118.443523   \n",
       "20629       Los Angeles  ...  34.0712 -118.443523   \n",
       "20685       Los Angeles  ...  34.0712 -118.443523   \n",
       "\n",
       "                     HCPCS_category          HCPCS_sub category  \\\n",
       "20647  New patient outpatient visit  , total time 60-74 minutes   \n",
       "20578  New patient outpatient visit  , total time 60-74 minutes   \n",
       "20649  New patient outpatient visit  , total time 60-74 minutes   \n",
       "20629  New patient outpatient visit  , total time 60-74 minutes   \n",
       "20685  New patient outpatient visit  , total time 60-74 minutes   \n",
       "\n",
       "      Rndrng_Prvdr_Zip5_str HCPCS_Cd_str Rndrng_Prvdr_Type_str  \\\n",
       "20647                 90095        99205             Neurology   \n",
       "20578                 90095        99205             Neurology   \n",
       "20649                 90095        99205             Neurology   \n",
       "20629                 90095        99205             Neurology   \n",
       "20685                 90095        99205             Neurology   \n",
       "\n",
       "       combined_cat_column Reverse_Avg_Sbmtd_Chrg  \\\n",
       "20647  9009599205Neurology                7651.00   \n",
       "20578  9009599205Neurology                7536.00   \n",
       "20649  9009599205Neurology                7289.48   \n",
       "20629  9009599205Neurology                6641.00   \n",
       "20685  9009599205Neurology                6641.00   \n",
       "\n",
       "      sum_beneficiary_days_and_rev_avg_charge  \n",
       "20647                                 7866.00  \n",
       "20578                                 7579.00  \n",
       "20649                                 7314.48  \n",
       "20629                                 6801.00  \n",
       "20685                                 6783.00  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7511d840-bd57-4909-8d6e-4cc667505dd8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'combined_cat_column'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mE:\\Python\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'combined_cat_column'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_5_percent_binary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Iterate through each unique value in 'field1'\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m unique_value \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcombined_cat_column\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39munique():\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#     # Filter the DataFrame for rows with the current unique value in 'field1'\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     subset \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_cat_column\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m unique_value]\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(subset)\n",
      "File \u001b[1;32mE:\\Python\\Lib\\site-packages\\pandas\\core\\frame.py:3896\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3896\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3898\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mE:\\Python\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3795\u001b[0m     ):\n\u001b[0;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'combined_cat_column'"
     ]
    }
   ],
   "source": [
    "\n",
    "df3=df2\n",
    "\n",
    "# Find the maximum rank value in the 'Tot_Bene_Day_Srvcs_rank_natop2' column\n",
    "max_rank_value = df3['Avg_Sbmtd_Chrg'].max()\n",
    "\n",
    "# Create a new column with values in reverse order\n",
    "df3['Reverse_Avg_Sbmtd_Chrg'] = max_rank_value - df3['Avg_Sbmtd_Chrg'] + 1\n",
    "\n",
    "# Create a new column 'Sum_Columns' by adding 'Column1' and 'Column2'\n",
    "df3['sum_beneficiary_days_and_rev_avg_charge'] = df3['Tot_Bene_Day_Srvcs'] + df3['Reverse_Avg_Sbmtd_Chrg']\n",
    "\n",
    "df3=df2\n",
    "\n",
    "# Find the maximum value in the column\n",
    "max_value = df3['sum_beneficiary_days_and_rev_avg_charge'].max()\n",
    "\n",
    "#--------------------------------------\n",
    "\n",
    "# Sort the DataFrame by 'field2' in descending order\n",
    "df3.sort_values(by='sum_beneficiary_days_and_rev_avg_charge', ascending=False, inplace=True)\n",
    "\n",
    "# Create a new column 'new_column' and initialize it with 0\n",
    "df['top_5_percent_binary'] = 0\n",
    "\n",
    "# Iterate through each unique value in 'field1'\n",
    "for unique_value in df['combined_cat_column'].unique():\n",
    "#     # Filter the DataFrame for rows with the current unique value in 'field1'\n",
    "    subset = df[df['combined_cat_column'] == unique_value]\n",
    "    print(subset)\n",
    "\n",
    "#     # Calculate the 95th percentile within this subset\n",
    "#     percentile_95 = subset['sum_beneficiary_days_and_rev_avg_charge'].quantile(0.95)\n",
    "\n",
    "#     # Set 'new_column' to 1 for rows within this subset where 'field2' is greater than or equal to the 95th percentile\n",
    "#     df.loc[df['sum_beneficiary_days_and_rev_avg_charge'] >= percentile_95, 'top_5_percent_binary'] = 1\n",
    "\n",
    "# # Reset the index\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# #--------------------------------------\n",
    "# df3.to_excel('E://GitHub//healthcare_provider_recommendation//df3_001_output.xlsx', index=False, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f397d9ed-c595-4000-868f-1a9d38793aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum rank value in the 'Tot_Bene_Day_Srvcs_rank_natop2' column\n",
    "max_rank_value = df3['Avg_Sbmtd_Chrg'].max()\n",
    "\n",
    "# Create a new column with values in reverse order\n",
    "df3['Reverse_Avg_Sbmtd_Chrg'] = max_rank_value - df3['Avg_Sbmtd_Chrg'] + 1\n",
    "\n",
    "# Create a new column 'Sum_Columns' by adding 'Column1' and 'Column2'\n",
    "df3['sum_beneficiary_days_and_rev_avg_charge'] = df3['Tot_Bene_Day_Srvcs'] + df3['Reverse_Avg_Sbmtd_Chrg']\n",
    "\n",
    "# # Calculate the percentiles (as quantiles between 0 and 1)\n",
    "# percentiles = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.01]\n",
    "\n",
    "# # Use the 'qcut' function to bin the data based on percentiles\n",
    "# df3['Bucket'] = pd.qcut(df3['sum_beneficiary_days_and_rev_avg_charge'], q=quantiles, labels=False, precision=0)\n",
    "# Check the unique values in the 'Bucket' column\n",
    "\n",
    "# # Define the number of bins (in this case, 10)\n",
    "# num_bins = 10\n",
    "\n",
    "# # Use the 'cut' function to create bins and assign each value to a bin\n",
    "# df3['Bucket_Normalized'] = pd.cut(df3['sum_beneficiary_days_and_rev_avg_charge'], bins=num_bins, labels=False)\n",
    "\n",
    "df3.to_excel('E://GitHub//healthcare_provider_recommendation/df3_002_output.xlsx', index=False, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f2f35c2f-9471-4c50-a2b7-4d5df788b394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Find the maximum value in the column\n",
    "max_value = df3['sum_beneficiary_days_and_rev_avg_charge'].max()\n",
    "\n",
    "# # Use the apply function to create a new column with 1 for the max value and 0 for others\n",
    "# df3['Binary_sum_beneficiary_days_and_rev_avg_charge'] = df3['sum_beneficiary_days_and_rev_avg_charge'].apply(lambda x: 1 if x == max_value else 0)\n",
    "\n",
    "# # Calculate the 95th percentile\n",
    "# percentile_95 = df3['sum_beneficiary_days_and_rev_avg_charge'].quantile(0.95)\n",
    "\n",
    "# # Use the apply function to create a new column\n",
    "# df3['Binary_sum_beneficiary_days_and_rev_avg_charge'] = df3['sum_beneficiary_days_and_rev_avg_charge'].apply(\n",
    "#     lambda x: 1 if x >= percentile_95 else 0\n",
    "# )\n",
    "\n",
    "#--------------------------------------\n",
    "\n",
    "# Sort the DataFrame by 'field2' in descending order\n",
    "df3.sort_values(by='sum_beneficiary_days_and_rev_avg_charge', ascending=False, inplace=True)\n",
    "\n",
    "# Create a new column 'new_column' and initialize it with 0\n",
    "df['top_5_percent_binary'] = 0\n",
    "\n",
    "# Iterate through each unique value in 'field1'\n",
    "for unique_value in df['combined_cat_column'].unique():\n",
    "    # Filter the DataFrame for rows with the current unique value in 'field1'\n",
    "    subset = df[df['combined_cat_column'] == unique_value]\n",
    "\n",
    "    # Calculate the 95th percentile within this subset\n",
    "    percentile_95 = subset['sum_beneficiary_days_and_rev_avg_charge'].quantile(0.95)\n",
    "\n",
    "    # Set 'new_column' to 1 for rows within this subset where 'field2' is greater than or equal to the 95th percentile\n",
    "    df.loc[df['sum_beneficiary_days_and_rev_avg_charge'] >= percentile_95, 'top_5_percent_binary'] = 1\n",
    "\n",
    "# Reset the index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "#--------------------------------------\n",
    "\n",
    "# Assuming you have already loaded and preprocessed your data\n",
    "selected_columns=[ 'Rndrng_Prvdr_Last_Org_Name','Rndrng_Prvdr_First_Name','Rndrng_Prvdr_MI', 'Rndrng_Prvdr_Crdntls', 'Rndrng_Prvdr_Gndr',\n",
    "       'Rndrng_Prvdr_Ent_Cd', 'Rndrng_Prvdr_St1', 'Rndrng_Prvdr_St2',\n",
    "       'Rndrng_Prvdr_City', 'Rndrng_Prvdr_State_Abrvtn',\n",
    "       'Rndrng_Prvdr_State_FIPS', 'Rndrng_Prvdr_Zip5', 'Rndrng_Prvdr_RUCA',\n",
    "       'Rndrng_Prvdr_RUCA_Desc', 'Rndrng_Prvdr_Cntry', 'Rndrng_Prvdr_Type',\n",
    "       'Rndrng_Prvdr_Mdcr_Prtcptg_Ind', 'HCPCS_Cd', 'HCPCS_Desc',\n",
    "       'HCPCS_Drug_Ind', 'Place_Of_Srvc', 'Tot_Benes', 'Tot_Srvcs',\n",
    "       'Tot_Bene_Day_Srvcs', 'Avg_Sbmtd_Chrg', 'Avg_Mdcr_Alowd_Amt',\n",
    "       'Avg_Mdcr_Pymt_Amt', 'Avg_Mdcr_Stdzd_Amt']\n",
    "\n",
    "# List of columns to label encode\n",
    "columns_to_encode = selected_columns\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "df4=df3\n",
    "\n",
    "# # Encode the selected columns\n",
    "# for column in columns_to_encode:\n",
    "#     df4[column] = label_encoder.fit_transform(df4[column])\n",
    "df4.to_excel('E://REVA//Healthcare Providers Data For Anomaly Detection//Medicare1//df4_001_output.xlsx', index=False, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c3b286d7-27e9-44b9-97a0-9649a824dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Iterate through columns\n",
    "for column in df4.columns:\n",
    "    # # Convert the column to numeric if possible, ignoring errors\n",
    "    # df4[column] = pd.to_numeric(df4[column], errors='coerce')\n",
    "    \n",
    "    # Find the least non-null value in the column\n",
    "    least_value = df4[column].min(skipna=True)\n",
    "    \n",
    "    unique_values = df4[column].nunique()\n",
    "    data_type = df4[column].dtype\n",
    "\n",
    "    # Fill null values with the least value\n",
    "    df4[column].fillna(least_value, inplace=True)\n",
    "    \n",
    "    # Drop the column if there's only one unique value\n",
    "    if unique_values == 1:\n",
    "        df4.drop(column, axis=1, inplace=True)\n",
    "    \n",
    "    # Apply label encoding if there are multiple unique values and the data type is object (string)\n",
    "    if unique_values > 1 and data_type == 'object':\n",
    "        label_encoder = LabelEncoder()\n",
    "        df4[column] = label_encoder.fit_transform(df4[column])\n",
    "       \n",
    "\n",
    "df4.to_excel('E://REVA//Healthcare Providers Data For Anomaly Detection//Medicare1//df4_27_output.xlsx', index=False, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5990e3ed-14d0-48b5-8e42-bd7281596db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # df4.drop([selected_columns], axis=1)\n",
    "\n",
    "# # # Split the data into features (X) and the target (y)\n",
    "X = df4.drop('Binary_sum_beneficiary_days_and_rev_avg_charge', axis=1)\n",
    "y = df4['Binary_sum_beneficiary_days_and_rev_avg_charge']\n",
    "\n",
    "# df4.to_excel('E://REVA//Healthcare Providers Data For Anomaly Detection//Medicare1//df4_23_output.xlsx', index=False, engine='openpyxl')\n",
    "# X.to_excel('E://REVA//Healthcare Providers Data For Anomaly Detection//Medicare1//x_23_output.xlsx', index=False, engine='openpyxl')\n",
    "# y.to_excel('E://REVA//Healthcare Providers Data For Anomaly Detection//Medicare1//y_23_output.xlsx', index=False, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f145d167-8987-436a-8c90-e4129cba5b81",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 30\u001b[0m\n\u001b[0;32m     20\u001b[0m classifiers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogistic Regression\u001b[39m\u001b[38;5;124m'\u001b[39m: LogisticRegression(),\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m'\u001b[39m: RandomForestClassifier(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGradient Boosting\u001b[39m\u001b[38;5;124m'\u001b[39m: GradientBoostingClassifier()\n\u001b[0;32m     27\u001b[0m }\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, classifier \u001b[38;5;129;01min\u001b[39;00m classifiers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 30\u001b[0m     \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     32\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n",
      "File \u001b[1;32mE:\\Python\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Python\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1207\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1205\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m-> 1207\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1209\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1213\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1214\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1215\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[1;32mE:\\Python\\Lib\\site-packages\\sklearn\\base.py:621\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    619\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 621\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mE:\\Python\\Lib\\site-packages\\sklearn\\utils\\validation.py:1147\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1142\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1145\u001b[0m     )\n\u001b[1;32m-> 1147\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1163\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1165\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mE:\\Python\\Lib\\site-packages\\sklearn\\utils\\validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    955\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    956\u001b[0m         )\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 959\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mE:\\Python\\Lib\\site-packages\\sklearn\\utils\\validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Python\\Lib\\site-packages\\sklearn\\utils\\validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m     )\n\u001b[1;32m--> 173\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.to_excel('E://REVA//Healthcare Providers Data For Anomaly Detection//Medicare1//X_train26.xlsx', index=False, engine='openpyxl')\n",
    "X_test.to_excel('E://REVA//Healthcare Providers Data For Anomaly Detection//Medicare1//X_test26.xlsx', index=False, engine='openpyxl')\n",
    "y_train.to_excel('E://REVA//Healthcare Providers Data For Anomaly Detection//Medicare1//y_train26.xlsx', index=False, engine='openpyxl')\n",
    "y_test.to_excel('E://REVA//Healthcare Providers Data For Anomaly Detection//Medicare1//y_test26.xlsx', index=False, engine='openpyxl')\n",
    "\n",
    "# Initialize and train various classification algorithms\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'{name} Accuracy: {accuracy}')\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1d7bb64-c924-4c05-970d-b853251d292c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   field1  field2  new_column\n",
      "19      B     100           0\n",
      "18      B      90           0\n",
      "17      B      80           0\n",
      "16      B      70           0\n",
      "15      B      60           0\n",
      "14      B      50           0\n",
      "13      B      40           0\n",
      "12      B      30           0\n",
      "11      B      20           0\n",
      "10      B      10           0\n",
      "  field1  field2  new_column\n",
      "9      A     100           0\n",
      "8      A      90           0\n",
      "7      A      80           0\n",
      "6      A      70           0\n",
      "5      A      60           0\n",
      "4      A      50           0\n",
      "3      A      40           0\n",
      "2      A      30           0\n",
      "1      A      20           0\n",
      "0      A      10           0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brajesh\\AppData\\Local\\Temp\\ipykernel_12372\\596787639.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset['field2'] = df['field2'].apply(lambda x: 1 if x >= percentile_95 else 0)\n",
      "C:\\Users\\brajesh\\AppData\\Local\\Temp\\ipykernel_12372\\596787639.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset['field2'] = df['field2'].apply(lambda x: 1 if x >= percentile_95 else 0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {\n",
    "    'field1': ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B'],\n",
    "    'field2': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Sort the DataFrame by 'field2' in descending order\n",
    "df.sort_values(by='field2', ascending=False, inplace=True)\n",
    "\n",
    "# Calculate the total number of rows\n",
    "total_rows = len(df)\n",
    "\n",
    "# Calculate the number of rows that correspond to the top 5%\n",
    "top_5_percent = int(total_rows * 0.05)\n",
    "\n",
    "# Create a new column 'new_column' and initialize it with 0\n",
    "df['new_column'] = 0\n",
    "\n",
    "# Iterate through each unique value in 'field1'\n",
    "for unique_value in df['field1'].unique():\n",
    "    # Filter the DataFrame for rows with the current unique value in 'field1'\n",
    "    subset = df[df['field1'] == unique_value]\n",
    "    print(subset)\n",
    "\n",
    "    # Calculate the 95th percentile\n",
    "    percentile_95 = subset['field2'].quantile(0.75)\n",
    "    \n",
    "    # Use the apply function to create a new column\n",
    "    subset['field2'] = df['field2'].apply(lambda x: 1 if x >= percentile_95 else 0)\n",
    "\n",
    "    # # Set the 'new_column' to 1 for the top 5% rows within this subset\n",
    "    # subset.iloc[:top_5_percent, subset.columns.get_loc('new_column')] = 1\n",
    "\n",
    "# # Reset the index\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # Display the modified DataFrame\n",
    "# print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1743f034-b011-4652-b610-80ed37422c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   field1  field2  new_column\n",
      "0       B     100           1\n",
      "1       A     100           1\n",
      "2       A      90           0\n",
      "3       B      90           0\n",
      "4       A      80           0\n",
      "5       B      80           0\n",
      "6       A      70           0\n",
      "7       B      70           0\n",
      "8       A      60           0\n",
      "9       B      60           0\n",
      "10      A      50           0\n",
      "11      B      50           0\n",
      "12      A      40           0\n",
      "13      B      40           0\n",
      "14      A      30           0\n",
      "15      B      30           0\n",
      "16      A      20           0\n",
      "17      B      20           0\n",
      "18      A      10           0\n",
      "19      B      10           0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {\n",
    "    'field1': ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B'],\n",
    "    'field2': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Sort the DataFrame by 'field2' in descending order\n",
    "df.sort_values(by='field2', ascending=False, inplace=True)\n",
    "\n",
    "# Create a new column 'new_column' and initialize it with 0\n",
    "df['new_column'] = 0\n",
    "\n",
    "# Iterate through each unique value in 'field1'\n",
    "for unique_value in df['field1'].unique():\n",
    "    # Filter the DataFrame for rows with the current unique value in 'field1'\n",
    "    subset = df[df['field1'] == unique_value]\n",
    "\n",
    "    # Calculate the 95th percentile within this subset\n",
    "    percentile_95 = subset['field2'].quantile(0.95)\n",
    "\n",
    "    # Set 'new_column' to 1 for rows within this subset where 'field2' is greater than or equal to the 95th percentile\n",
    "    df.loc[df['field2'] >= percentile_95, 'new_column'] = 1\n",
    "\n",
    "# Reset the index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the modified DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a88b2b8-ac69-4401-a294-44e9810a79d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
